\# Dataset Description

\#\# Input Data

\#\#\# evaluation-set  
\- \*\*subject:\*\* The specific subject or domain being evaluated (e.g., mathematics, physics, etc.).  
\- \*\*question:\*\* The question being posed in the evaluation.  
\- \*\*contexts:\*\* Additional information or background related to the question.  
\- \*\*correct\_answer:\*\* The correct answer to the question.  
\- \*\*model\_answer:\*\* The answer generated by the model.

\#\#\# theory-data  
\- \*\*question\*\*  
\- \*\*correct\_answer:\*\* The correct theoretical answer to the question.  
\- \*\*model\_solution:\*\* The solution provided by the model.  
\- \*\*source\_document\_0\_page\_content:\*\* Content from the first source document used for context.  
\- \*\*source\_document\_1\_page\_content:\*\* Content from the second source document used for context.  
\- \*\*source\_document\_2\_page\_content:\*\* Content from the third source document used for context.

\#\#\# math-data  
\- \*\*question\*\*  
\- \*\*correct\_answer:\*\* The correct mathematical answer to the question.  
\- \*\*model\_solution:\*\* The solution provided by the model.

\#\# Output Data

\#\#\# theory\_scores\_flow1  
\- \*\*context\_link {0,1}:\*\* Indicates whether the context is linked (1) or not (0).  
\- \*\*correct\_answer\_link {0,1}:\*\* Indicates whether the correct answer is linked (1) or not (0).  
\- \*\*context\_precision \[0,1\]:\*\* Precision of the context, ranging from 0 to 1\.  
\- \*\*context\_recall \[0,1\]:\*\* Recall of the context, ranging from 0 to 1\.  
\- \*\*faithfulness \[0,1\]:\*\* Faithfulness of the answer to the context, ranging from 0 to 1\.  
\- \*\*answer\_relevancy \[0,1\]:\*\* Relevancy of the answer, ranging from 0 to 1\.  
\- \*\*total\_questions:\*\* Total number of questions evaluated.

\#\#\# math\_scores\_flow1  
\- \*\*context\_link {0,1}:\*\* Indicates whether the context is linked (1) or not (0).  
\- \*\*correct\_answer\_link {0,1}:\*\* Indicates whether the correct answer is linked (1) or not (0).  
\- \*\*final\_answer\_score {0,1}:\*\* Score of the final answer, either correct (1) or incorrect (0).  
\- \*\*math\_accuracy {0,1}:\*\* Accuracy of the mathematical solution, ranging from 0 to 1\.  
\- \*\*total\_questions:\*\* Total number of questions evaluated.

\#\#\# theory\_scores\_flow2  
\- \*\*context\_recall \[0,1\]:\*\* Recall of the context, ranging from 0 to 1\.  
\- \*\*faithfulness \[0,1\]:\*\* Faithfulness of the answer to the context, ranging from 0 to 1\.  
\- \*\*answer\_relevancy \[0,1\]:\*\* Relevancy of the answer, ranging from 0 to 1\.  
\- \*\*total\_questions:\*\* Total number of questions evaluated.

\#\#\# math\_scores\_flow2  
\- \*\*final\_answer\_score {0,1}:\*\* Score of the final answer, either correct (1) or incorrect (0).  
\- \*\*math\_accuracy {0,1}:\*\* Accuracy of the mathematical solution, ranging from 0 to 1\.  
\- \*\*total\_questions:\*\* Total number of questions evaluated.

\#\# Metrics

\#\#\# RAGAS  
\- \*\*context precision:\*\* Measure of how accurately the provided context is used in generating the answer.  
\- \*\*context recall:\*\* Measure of how well the context covers the necessary information to answer the question.  
\- \*\*faithfulness:\*\* Measure of how faithfully the answer reflects the information in the context.  
\- \*\*answer relevancy:\*\* Measure of how relevant the answer is to the question.

\#\#\# Additional Metrics  
\- \*\*context\_link:\*\* Binary metric indicating whether the context is appropriately linked (1) or not (0).  
\- \*\*correct\_answer\_link:\*\* Binary metric indicating whether the correct answer is appropriately linked (1) or not (0).  
\- \*\*final\_answer\_score:\*\* Binary metric indicating whether the final answer is correct (1) or incorrect (0).  
\- \*\*math\_accuracy:\*\* Measure of the accuracy of mathematical solutions, ranging from 0 to 1\.  
